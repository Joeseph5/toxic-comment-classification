{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import GRU,GlobalMaxPool1D\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cdac2e867bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# 按空格分词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('R'):\n",
    "            yield wnl.lemmatize(word, pos='r')\n",
    "            \n",
    "        else:\n",
    "            yield word\n",
    "\n",
    "\n",
    "def msgProcessing(raw_msg):\n",
    "    m_w=[]\n",
    "    words2=[]\n",
    "    raw_msg=str(raw_msg)\n",
    "    raw_msg = str(raw_msg.lower())\n",
    "    raw_msg=re.sub(r'[^a-zA-Z]', ' ', raw_msg)\n",
    "    \n",
    "    words=raw_msg.lower().split()\n",
    "    #Remove words with length lesser than 3\n",
    "    for i in words:\n",
    "        if len(i)>=0:\n",
    "            words2.append(i)\n",
    "    stops=set(stopwords.words('english'))\n",
    "    m_w=\" \".join([w for w in words2])\n",
    "    return(\" \".join(lemmatize_all(m_w)))\n",
    "\n",
    "\n",
    "def helperFunction(df):\n",
    "    print (\"Data Preprocessing!!!\")\n",
    "    cols=['comment_text']\n",
    "    df=df[cols]\n",
    "    df.comment_text.replace({r'[^\\x00-\\x7F]+':''},regex=True,inplace=True)\n",
    "    num_msg=df[cols].size\n",
    "    clean_msg=[]\n",
    "    for i in range(0,num_msg):\n",
    "        clean_msg.append(msgProcessing(df['comment_text'][i]))\n",
    "    df['Processed_msg']=clean_msg\n",
    "    X=df['Processed_msg']\n",
    "    print (\"Data Preprocessing Ends!!!\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def embedding(train,test):\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(train)\n",
    "    t=len(tokenizer.word_index)+1\n",
    "    trainsequences = tokenizer.texts_to_sequences(train)\n",
    "    traindata = pad_sequences(trainsequences, maxlen=100)\n",
    "    testsequences = tokenizer.texts_to_sequences(test)\n",
    "    testdata = pad_sequences(testsequences, maxlen=100)\n",
    "    return traindata, testdata,tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 加入Glove预训练词\n",
    "EMBEDDING_FILE = 'words_vector/glove.840B.300d.txt'\n",
    "embeddings_index = {}\n",
    "\n",
    "# 读取glove文件\n",
    "f = open(EMBEDDING_FILE, encoding = 'utf-8')\n",
    "for line in f:\n",
    "\n",
    "    # 按空格分词\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "# 关闭glove文件\n",
    "f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "def x1(tokenizer):\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(10000, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, 300))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= 10000: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "\n",
    "def getTarget(y):\n",
    "    ytrain=y[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "    return ytrain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multi_channel_model(xtrain, ytrain):\n",
    "    batch_size=500\n",
    "    epochs=2\n",
    "    input1=Input(shape=(100,))\n",
    "    embedding1=Embedding(10000, 300, weights=[embedding_matrix])(input1)\n",
    "    conv1=Conv1D(filters=32, kernel_size=3, activation='relu')(embedding1)\n",
    "    drop1= Dropout(0.4)(conv1)\n",
    "    pool1=MaxPooling1D(pool_size=4)(drop1)\n",
    "    gru1= GRU(100, dropout=0.2, recurrent_dropout=0.2)(pool1)\n",
    "    \n",
    "    \n",
    "    input2=Input(shape=(100,))\n",
    "    embedding2=Embedding(10000, 300, weights=[embedding_matrix])(input2)\n",
    "    conv2=Conv1D(filters=32, kernel_size=4, activation='relu')(embedding2)\n",
    "    drop2= Dropout(0.45)(conv2)\n",
    "    pool2=MaxPooling1D(pool_size=4)(drop2)\n",
    "    gru2= GRU(100, dropout=0.2, recurrent_dropout=0.2)(pool2)\n",
    "    \n",
    "    \n",
    "    input3=Input(shape=(100,))\n",
    "    embedding3=Embedding(10000, 300, weights=[embedding_matrix])(input3)\n",
    "    conv3=Conv1D(filters=32, kernel_size=5, activation='relu')(embedding3)\n",
    "    drop3= Dropout(0.5)(conv3)\n",
    "    pool3=MaxPooling1D(pool_size=4)(drop3)\n",
    "    gru3= GRU(100, dropout=0.2, recurrent_dropout=0.2)(pool3)\n",
    "    \n",
    "    \n",
    "    merged= concatenate([gru1,gru2,gru3])\n",
    "    dense1 = Dense(100, activation='relu')(merged)\n",
    "    outputs = Dense(6, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit([xtrain,xtrain,xtrain],ytrain,batch_size=batch_size,epochs=epochs)\n",
    "    model.save(\"MultiChannel.h5\")\n",
    "\n",
    "def validate(xtest):\n",
    "    model=load_model(\"MultiChannel.h5\")\n",
    "    pred=model.predict([xtest,xtest,xtest])\n",
    "    return pred\n",
    "\n",
    "\n",
    "def saveCSV(ytest):\n",
    "    sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "    sample_submission[classes] = ytest\n",
    "    sample_submission.to_csv(\"Multichanneltoxic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "df= pd.read_csv(\"data/train.csv\",encoding=\"ISO-8859-1\")\n",
    "df2=pd.read_csv(\"data/test.csv\",encoding=\"ISO-8859-1\")\n",
    "df2['comment_text'].fillna('Missing',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/generic.py:4619: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing Ends!!!\n",
      "Data Preprocessing!!!\n",
      "Data Preprocessing Ends!!!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (300) into shape (50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d20599923b18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-71213076823c>\u001b[0m in \u001b[0;36mx1\u001b[0;34m(tokenizer)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0membedding_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (300) into shape (50)"
     ]
    }
   ],
   "source": [
    "X=helperFunction(df)\n",
    "X2=helperFunction(df2)\n",
    "\n",
    "xtrain,xtest,tokenizer=embedding(X,X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=x1(tokenizer)\n",
    "ytrain=getTarget(df[classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "159571/159571 [==============================] - 66s 414us/step - loss: 0.0851 - acc: 0.9721\n",
      "Epoch 2/2\n",
      "159571/159571 [==============================] - 63s 395us/step - loss: 0.0511 - acc: 0.9811\n"
     ]
    }
   ],
   "source": [
    "multi_channel_model(xtrain,ytrain)\n",
    "ytest=validate(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveCSV(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
