{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, SpatialDropout1D, concatenate\n",
    "from keras.layers import Bidirectional, GRU\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# read data\n",
    "train = pd.read_csv('data/train.csv').fillna(' ')\n",
    "test = pd.read_csv('data/test.csv').fillna(' ')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# 单独保存comment_text\n",
    "train_text = train['comment_text'].str.lower()\n",
    "test_text = test['comment_text'].str.lower()\n",
    "# 获得y_train\n",
    "y_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "# 连接所有文字用于分词\n",
    "all_text = pd.concat([train_text, test_text], axis = 0, ignore_index = True)\n",
    "# glove预训练数据\n",
    "EMBEDDING_FILE = 'words_vector/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      0             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0\n",
       "5      0             0        0       0       0              0\n",
       "6      1             1        1       0       1              0\n",
       "7      0             0        0       0       0              0\n",
       "8      0             0        0       0       0              0\n",
       "9      0             0        0       0       0              0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始数据可视化分析\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 数据预处理\\n# all_text = all_text[:100]\\ndef lemmatize_all(sentence):\\n    wnl = WordNetLemmatizer()\\n    for word, tag in pos_tag(word_tokenize(sentence)):\\n        if tag.startswith(\"NN\"):\\n            yield wnl.lemmatize(word, pos=\\'n\\')\\n        elif tag.startswith(\\'VB\\'):\\n            yield wnl.lemmatize(word, pos=\\'v\\')\\n        elif tag.startswith(\\'JJ\\'):\\n            yield wnl.lemmatize(word, pos=\\'a\\')\\n        elif tag.startswith(\\'R\\'):\\n            yield wnl.lemmatize(word, pos=\\'r\\')\\n            \\n        else:\\n            yield word\\n\\ndef text_cleaned_process(text_raw):    \\n    text_raw = str(text_raw)\\n    text_raw = str(text_raw.lower())\\n    text_raw = re.sub(r\\'[^a-zA-Z]\\', \\' \\', text_raw)\\n    \\n    words = text_raw.split()\\n    \\n    # 移除长度小于3的词语\\n    words2 = []\\n    for i in words:\\n        if len(i) >= 0:\\n            words2.append(i)\\n    # 去停止词\\n    stops = set(stopwords.words(\\'english\\'))\\n    \\n    result_text = []\\n    result_text = \" \".join([w for w in words2 if not w in stops])\\n    \\n    return(\" \".join(lemmatize_all(result_text)))\\n\\n# 去掉数字\\nall_text.replace({r\\'[^\\x00-\\x7f]+\\':\\'\\'},regex=True,inplace=True)\\n\\nnum_all_text = all_text.size\\n\\n# 输出清洗后的数据\\nall_text_cleaned = []\\n\\nfor i in range(0, num_all_text):\\n    all_text_cleaned.append(text_cleaned_process(all_text[i]))\\n\\n# 构建pd形式\\nall_text_cleaned = pd.Series(all_text_cleaned)\\n    \\n\\nspecial_character_removal=re.compile(r\\'[^a-z\\\\d ]\\',re.IGNORECASE)\\nreplace_numbers=re.compile(r\\'\\\\d+\\',re.IGNORECASE)\\n\\ndef text_to_words(text):\\n    # 去掉一些特殊字符\\n    text = text.split()\\n    text = \\' \\'.join(text)\\n    \\n    # 进一步去掉特殊字母\\n    text=special_character_removal.sub(\\'\\',text)\\n    \\n    # 用空格代替数字\\n    text=replace_numbers.sub(\\'\\', text)\\n    \\n    # 去停止词\\n    stops = set(stopwords.words(\"english\"))\\n    text = text.split()\\n    text = [w for w in text if not w in stops]\\n\\n    # 去时态后缀\\n    \\'\\'\\'\\n    stemmer = SnowballStemmer(\\'english\\')\\n    stemmed_words = [stemmer.stem(word) for word in text]\\n    \\'\\'\\'\\n    text = \\' \\'.join(text)\\n     \\n    return text\\n\\n# 输出清洗后数据\\nall_text_cleaned = []\\nfor text in all_text:\\n    all_text_cleaned.append(text_to_words(text))\\n\\n# 构建pd格式\\nall_text_cleaned = pd.Series(all_text_cleaned)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 数据预处理\n",
    "# all_text = all_text[:100]\n",
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('R'):\n",
    "            yield wnl.lemmatize(word, pos='r')\n",
    "            \n",
    "        else:\n",
    "            yield word\n",
    "\n",
    "def text_cleaned_process(text_raw):    \n",
    "    text_raw = str(text_raw)\n",
    "    text_raw = str(text_raw.lower())\n",
    "    text_raw = re.sub(r'[^a-zA-Z]', ' ', text_raw)\n",
    "    \n",
    "    words = text_raw.split()\n",
    "    \n",
    "    # 移除长度小于3的词语\n",
    "    words2 = []\n",
    "    for i in words:\n",
    "        if len(i) >= 0:\n",
    "            words2.append(i)\n",
    "    # 去停止词\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    result_text = []\n",
    "    result_text = \" \".join([w for w in words2 if not w in stops])\n",
    "    \n",
    "    return(\" \".join(lemmatize_all(result_text)))\n",
    "\n",
    "# 去掉数字\n",
    "all_text.replace({r'[^\\x00-\\x7F]+':''},regex=True,inplace=True)\n",
    "\n",
    "num_all_text = all_text.size\n",
    "\n",
    "# 输出清洗后的数据\n",
    "all_text_cleaned = []\n",
    "\n",
    "for i in range(0, num_all_text):\n",
    "    all_text_cleaned.append(text_cleaned_process(all_text[i]))\n",
    "\n",
    "# 构建pd形式\n",
    "all_text_cleaned = pd.Series(all_text_cleaned)\n",
    "    \n",
    "\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_words(text):\n",
    "    # 去掉一些特殊字符\n",
    "    text = text.split()\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    # 进一步去掉特殊字母\n",
    "    text=special_character_removal.sub('',text)\n",
    "    \n",
    "    # 用空格代替数字\n",
    "    text=replace_numbers.sub('', text)\n",
    "    \n",
    "    # 去停止词\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = text.split()\n",
    "    text = [w for w in text if not w in stops]\n",
    "\n",
    "    # 去时态后缀\n",
    "    '''\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    '''\n",
    "    text = ' '.join(text)\n",
    "     \n",
    "    return text\n",
    "\n",
    "# 输出清洗后数据\n",
    "all_text_cleaned = []\n",
    "for text in all_text:\n",
    "    all_text_cleaned.append(text_to_words(text))\n",
    "\n",
    "# 构建pd格式\n",
    "all_text_cleaned = pd.Series(all_text_cleaned)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始数据可视化分析\n",
    "# all_text = all_text[:100]\n",
    "# print(\"Len of all_text_cleaned:\", len(all_text_cleaned))\n",
    "\n",
    "# print(\"Text[0] before cleaned: \\n\", all_text[6])\n",
    "# print(\"Text[0] after cleaned: \\n\", all_text_cleaned[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 394787\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "MAX_NUM_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# 分词\n",
    "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "sequences = tokenizer.texts_to_sequences(all_text)\n",
    "\n",
    "# 分词完成\n",
    "# Pads sequences to the same length， return(len(sequence, maxlen))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "# A dictionary of words and their uniquely assigned integers\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of Unique Tokens',len(word_index))\n",
    "\n",
    "# summarize what was learned\n",
    "# print(tokenizer.word_counts) # A dictionary of words and their counts\n",
    "# print(tokenizer.document_count) # A dictionary of words and how many documents each appeared in.\n",
    "# print(tokenizer.word_docs) # An integer count of the total number of documents that were used to fit the Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of x_train:  159571\n",
      "Len of y_train:  159571\n"
     ]
    }
   ],
   "source": [
    "# 重塑train与test数据\n",
    "x_train = data[:len(train_text)]\n",
    "x_test = data[len(train_text):]\n",
    "\n",
    "print(\"Len of x_train: \", len(x_train))\n",
    "print(\"Len of y_train: \", len(y_train))\n",
    "\n",
    "# 拆分train数据\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.15, random_state = 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 2195892 word vectors in glove.840B.300d.\n"
     ]
    }
   ],
   "source": [
    "# 加入Glove预训练词\n",
    "embeddings_index = {}\n",
    "\n",
    "# 读取glove文件\n",
    "f = open(EMBEDDING_FILE, encoding = 'utf-8')\n",
    "for line in f:\n",
    "\n",
    "    # 按空格分词\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "# 关闭glove文件\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in glove.840B.300d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# 载入预训练词向量作为Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建CNN模型\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype = 'int32')\n",
    "# 加入词向量\n",
    "embedded_sequence = embedding_layer(sequence_input)\n",
    "\n",
    "x = SpatialDropout1D(0.2)(embedded_sequence)\n",
    "\n",
    "x = Bidirectional(GRU(128, return_sequences = True, unroll = True))(x)\n",
    "# model0\n",
    "conv_0 = Conv1D(128, 1, kernel_initializer='normal', activation = 'relu')(x)\n",
    "drop_0= Dropout(0.4)(conv_0)\n",
    "max_pool0= MaxPooling1D(pool_size = 4)(drop_0)\n",
    "gru_0= GRU(100, dropout=0.2, recurrent_dropout=0.2)(max_pool0)\n",
    "# model1\n",
    "conv_1 = Conv1D(128, 2, kernel_initializer='normal', activation = 'relu')(x)\n",
    "drop_1= Dropout(0.45)(conv_1)\n",
    "max_pool1 = MaxPooling1D(pool_size = 4)(drop_1)\n",
    "gru_1= GRU(100, dropout=0.2, recurrent_dropout=0.2)(max_pool1)\n",
    "# model2\n",
    "conv_2 = Conv1D(128, 4, kernel_initializer='normal', activation = 'relu')(x)\n",
    "drop_2= Dropout(0.5)(conv_2)\n",
    "max_pool2 = MaxPooling1D(pool_size = 4)(drop_2)\n",
    "gru_2= GRU(100, dropout=0.2, recurrent_dropout=0.2)(max_pool2)\n",
    "\n",
    "# 模型融合\n",
    "conv_sum = concatenate([gru_0, gru_1, gru_2], axis = 1)\n",
    "\n",
    "# 压缩成对应6个标签\n",
    "#conv_sum = Flatten()(conv_sum)\n",
    "dense1 = Dense(100, activation='relu')(conv_sum)\n",
    "preds = Dense(6, activation = \"sigmoid\")(dense1)\n",
    "\n",
    "# 生成模型\n",
    "model = Model(inputs = sequence_input, outputs = preds)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135635 samples, validate on 23936 samples\n",
      "Epoch 1/5\n",
      " - 482s - loss: 0.0608 - acc: 0.9785 - val_loss: 0.0448 - val_acc: 0.9832\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.978918\n",
      "Epoch 2/5\n",
      " - 384s - loss: 0.0453 - acc: 0.9826 - val_loss: 0.0413 - val_acc: 0.9840\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986701\n",
      "Epoch 3/5\n",
      " - 384s - loss: 0.0422 - acc: 0.9835 - val_loss: 0.0399 - val_acc: 0.9843\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.988243\n",
      "Epoch 4/5\n",
      " - 384s - loss: 0.0399 - acc: 0.9843 - val_loss: 0.0395 - val_acc: 0.9844\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.988890\n",
      "Epoch 5/5\n",
      " - 383s - loss: 0.0380 - acc: 0.9849 - val_loss: 0.0391 - val_acc: 0.9845\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.989630\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "# 回调函数查看训练得分\n",
    "ROC_AUC = RocAucEvaluation(validation_data = (x_val, y_val), interval = 1)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size = 256, epochs = 5, validation_data = (x_val, y_val), verbose = 2, \n",
    "                   callbacks = [ROC_AUC])\n",
    "\n",
    "# 计算验证集数据得分\n",
    "# y_val_pred = model.predict(x_val, verbose = 2)\n",
    "# score = roc_auc_score(y_val, y_val_pred)\n",
    "# print(\"Validation data ROC-AUC score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 65s 426us/step\n"
     ]
    }
   ],
   "source": [
    "# test数据预测值\n",
    "y_prediction = model.predict(x_test, batch_size =1024, verbose = 1)\n",
    "\n",
    "# 生成语言各分类概率\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_prediction\n",
    "\n",
    "# 输出submission.csv\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.996242</td>\n",
       "      <td>3.177698e-01</td>\n",
       "      <td>0.979641</td>\n",
       "      <td>0.144245</td>\n",
       "      <td>0.918645</td>\n",
       "      <td>0.562831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.002732</td>\n",
       "      <td>4.287464e-06</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>8.561200e-06</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>1.246457e-06</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.047193</td>\n",
       "      <td>1.591872e-04</td>\n",
       "      <td>0.007537</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>1.380382e-06</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>1.660870e-06</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>0.555155</td>\n",
       "      <td>1.232047e-03</td>\n",
       "      <td>0.056318</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.108808</td>\n",
       "      <td>0.003820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>0.083074</td>\n",
       "      <td>2.363799e-05</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.008495</td>\n",
       "      <td>0.000262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>5.514399e-07</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.996242  3.177698e-01  0.979641  0.144245  0.918645   \n",
       "1  0000247867823ef7  0.002732  4.287464e-06  0.000414  0.000052  0.000103   \n",
       "2  00013b17ad220c46  0.001898  8.561200e-06  0.000472  0.000056  0.000108   \n",
       "3  00017563c3f7919a  0.000737  1.246457e-06  0.000145  0.000046  0.000043   \n",
       "4  00017695ad8997eb  0.047193  1.591872e-04  0.007537  0.000803  0.001472   \n",
       "5  0001ea8717f6de06  0.001328  1.380382e-06  0.000197  0.000038  0.000078   \n",
       "6  00024115d4cbde0f  0.003797  1.660870e-06  0.000212  0.000056  0.000106   \n",
       "7  000247e83dcc1211  0.555155  1.232047e-03  0.056318  0.001231  0.108808   \n",
       "8  00025358d4737918  0.083074  2.363799e-05  0.003229  0.000322  0.008495   \n",
       "9  00026d1092fe71cc  0.000383  5.514399e-07  0.000066  0.000015  0.000009   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.562831  \n",
       "1       0.000033  \n",
       "2       0.000025  \n",
       "3       0.000008  \n",
       "4       0.000275  \n",
       "5       0.000011  \n",
       "6       0.000014  \n",
       "7       0.003820  \n",
       "8       0.000262  \n",
       "9       0.000004  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初步检查输出结果\n",
    "submission.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
